# 1.Q-learning
## Q-learning 是 value-based 的方法。在 value based 的方法里面，我们 learn 的不是 policy，我们要 learn 的是一个 critic。
# 2.Double DQN
## Distributional Q-function 还蛮有道理的， 但是它没有红起来。你就发现说没有太多人真的在实现的时候用这个技术，可能一个原因就是它不好实现。Q-function 是 accumulated reward 的期望值，所以我们算出来的这个 Q value 其实是一个期望值。因为环境是有随机性的，在某一个 state 采取某一个 action 的时候，我们把所有的 reward 玩到游戏结束的时候所有的 reward 进行一个统计，你其实得到的是一个 distribution。也许在 reward 得到 0 的机率很高，在 -10 的概率比较低，在 +10 的概率比较低，但是它是一个 distribution。
